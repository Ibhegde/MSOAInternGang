{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pre-trained model on a sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFViTForImageClassification.\n",
      "\n",
      "All the layers of TFViTForImageClassification were initialized from the model checkpoint at google/vit-base-patch16-224.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFViTForImageClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: web site, website, internet site, site\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTImageProcessor, TFViTForImageClassification\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "\n",
    "img_path = r'TRAIN_IMAGES\\activity_category\\train\\Out of Home Media\\212345_00102046-002_164547-PT-Xmas-D15-KV-1276x1790-px.jpg'\n",
    "image = Image.open(img_path)\n",
    "\n",
    "feature_extractor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224', \n",
    "                                                        proxies={'https': 'proxy-ir.intel.com:912'})\n",
    "model = TFViTForImageClassification.from_pretrained('google/vit-base-patch16-224', \n",
    "                                                  proxies={'https': 'proxy-ir.intel.com:912'})\n",
    "\n",
    "inputs = feature_extractor(images=image, return_tensors=\"tf\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_class_idx = tf.math.argmax(logits, -1).numpy()[0]\n",
    "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the model and feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bhegde\\codes\\MSOAInternGang\\aie_venv\\lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTFeatureExtractor\n",
    "\n",
    "model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name_or_path, proxies={'https': 'proxy-ir.intel.com:912'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor, TFViTForImageClassification\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "stat_df = pd.read_csv(\"data/TRAIN_images_metadata.csv\")\n",
    "\n",
    "stat_df = stat_df.head(20)\n",
    "\n",
    "def process_image(image_file):\n",
    "    img_pil = Image.open(os.path.join(\"I:/TRAIN_IMAGES/\", image_file)).convert(\"RGB\")\n",
    "    # print(os.path.join(\"I:/TRAIN_IMAGES/\", image_file))\n",
    "    inp_img_enc = feature_extractor(img_pil, return_tensors='tf')\n",
    "    return inp_img_enc['pixel_values']\n",
    "\n",
    "stat_df['pixel_values'] = stat_df['image_name'].map(process_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "label_col = 'POA_attribution'\n",
    "\n",
    "train_data = stat_df[['pixel_values', label_col]].loc[:14]\n",
    "\n",
    "valid_data = stat_df[['pixel_values', label_col]].loc[15:]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': tf.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': tf.tensor([x['labels'] for x in batch])\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy_score(y_true = labels, y_pred = predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing TFViTForImageClassification: ['vit/pooler/dense/kernel:0', 'vit/pooler/dense/bias:0']\n",
      "- This IS expected if you are initializing TFViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nTrainer requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFTrainer\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 37\u001b[0m\n\u001b[0;32m     17\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[0;32m     18\u001b[0m   output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./vit-base-AIE-sample\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m   per_device_train_batch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m   load_best_model_at_end\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     32\u001b[0m )\n\u001b[0;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m Trainer\n\u001b[1;32m---> 37\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     38\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m     39\u001b[0m     args\u001b[39m=\u001b[39;49mtraining_args,\n\u001b[0;32m     40\u001b[0m     data_collator\u001b[39m=\u001b[39;49mcollate_fn,\n\u001b[0;32m     41\u001b[0m     compute_metrics\u001b[39m=\u001b[39;49mcompute_metrics,\n\u001b[0;32m     42\u001b[0m     train_dataset\u001b[39m=\u001b[39;49mtrain_data,\n\u001b[0;32m     43\u001b[0m     eval_dataset\u001b[39m=\u001b[39;49mvalid_data,\n\u001b[0;32m     44\u001b[0m     tokenizer\u001b[39m=\u001b[39;49mfeature_extractor,\n\u001b[0;32m     45\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\bhegde\\codes\\MSOAInternGang\\aie_venv\\lib\\site-packages\\transformers\\utils\\dummy_pt_objects.py:6954\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   6953\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> 6954\u001b[0m     requires_backends(\u001b[39mself\u001b[39;49m, [\u001b[39m\"\u001b[39;49m\u001b[39mtorch\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\bhegde\\codes\\MSOAInternGang\\aie_venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:1029\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1027\u001b[0m \u001b[39m# Raise an error for users who might not realize that classes without \"TF\" are torch-only\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m backends \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m backends \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_available() \u001b[39mand\u001b[39;00m is_tf_available():\n\u001b[1;32m-> 1029\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(PYTORCH_IMPORT_ERROR_WITH_TF\u001b[39m.\u001b[39mformat(name))\n\u001b[0;32m   1031\u001b[0m \u001b[39m# Raise the inverse error for PyTorch users trying to load TF classes\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m backends \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m backends \u001b[39mand\u001b[39;00m is_torch_available() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tf_available():\n",
      "\u001b[1;31mImportError\u001b[0m: \nTrainer requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFTrainer\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFViTForImageClassification\n",
    "from src.util import get_label_map\n",
    "\n",
    "poa_labels = get_label_map()[label_col]\n",
    "\n",
    "model = TFViTForImageClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=len(poa_labels),\n",
    "    id2label={v : k for k,v in poa_labels.items()},\n",
    "    label2id = poa_labels,\n",
    "    proxies={'https': 'proxy-ir.intel.com:912'}\n",
    ")\n",
    "\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./vit-base-AIE-sample\",\n",
    "  per_device_train_batch_size=16,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=10,\n",
    "  fp16=True,\n",
    "  save_steps=100,\n",
    "  eval_steps=100,\n",
    "  logging_steps=10,\n",
    "  learning_rate=1e-6,\n",
    "  save_total_limit=2,\n",
    "  remove_unused_columns=False,\n",
    "  push_to_hub=False,\n",
    "  report_to='tensorboard',\n",
    "  load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=valid_data,\n",
    "    tokenizer=feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate(valid_data)\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 5631 MB.\n",
      "GPU memory occupied: 5629 MB.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pynvml import *\n",
    "\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "torch.ones((1, 1)).to(\"cuda\")\n",
    "print_gpu_utilization()\n",
    "torch.cuda.empty_cache()\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n",
      "<torch.cuda.device object at 0x7fa7ffa0a310>\n",
      "Tesla V100-SXM2-16GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTImageProcessor, ViTForImageClassification, ViTConfig\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "img_path = r'../TRAIN_IMAGES/POA_attribution/train/Yes/100167_00092520-001_March Sale Countodwn.png'\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "# fe_config = ViTConfig.from_pretrained('../test-vit-base/POA_attribution/preprocessor_config.json')\n",
    "feature_extractor = ViTImageProcessor.from_pretrained('../test-vit-base/POA_attribution/')\n",
    "\n",
    "# m_config = ViTConfig.from_pretrained('../test-vit-base/POA_attribution/')\n",
    "model = ViTForImageClassification.from_pretrained('../test-vit-base/POA_attribution/')\n",
    "model = model.to('cuda')\n",
    "inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "inputs.to('cuda')\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "logits = logits.to('cpu')\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_class_idx = torch.argmax(logits, -1).numpy()[0]\n",
    "print(\"Predicted class:\", predicted_class_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ImageFolder' object has no attribute 'with_transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15574/2491294710.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprediction_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mpred_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mpred_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ImageFolder' object has no attribute 'with_transform'"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "import torch\n",
    "\n",
    "\n",
    "image_dir = \"../TRAIN_IMAGES/POA_attribution\"\n",
    "def transform_image(self, image_files):\n",
    "    inputs = self.feature_ext(\n",
    "        [x.convert(\"RGB\") for x in image_files[\"image\"]], return_tensors=\"pt\"\n",
    "    )\n",
    "    return inputs\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values}\n",
    "\n",
    "prediction_ds = datasets.ImageFolder(image_dir)\n",
    "pred_ds = prediction_ds.with_transform(transform_image)\n",
    "pred_loader = torch.utils.data.DataLoader(prediction_ds, batch_size = 32, shuffle=False, num_workers=4, collate_fn=collate_fn)\n",
    "\n",
    "for inputs in pred_loader:\n",
    "    print(inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aie_venv",
   "language": "python",
   "name": "aie_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "788814ecf7b2e71f9aac606fb3eee6b860c44d0c5a650a586b0a505ba47499ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
