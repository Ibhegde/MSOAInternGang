{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune pretrained model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the model and feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor\n",
    "\n",
    "model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(model_name_or_path, proxies={'https': 'proxy-ir.intel.com:912'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "\n",
    "# stat_df = pd.read_csv(\"data/TRAIN_images_metadata.csv\")\n",
    "\n",
    "# stat_df = stat_df.sample(20, random_state=8, ignore_index=True)\n",
    "\n",
    "def process_image(image_file):\n",
    "    img_pil = Image.open(os.path.join(\"I:/TRAIN_IMAGES/\", image_file)).convert(\"RGB\")\n",
    "    inp_img_enc = feature_extractor(img_pil, return_tensors='pt')\n",
    "    return inp_img_enc['pixel_values']\n",
    "\n",
    "# stat_df['pixel_values'] = stat_df['image_name'].map(process_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.util import get_label_map\n",
    "\n",
    "label_col = 'POA_attribution'\n",
    "\n",
    "labels_lst = get_label_map()[label_col]\n",
    "# stat_df[label_col] = stat_df[label_col].map(lambda el:labels_lst[el])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Torch ImageFolder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from src.util import get_data_set\n",
    "\n",
    "# train_data = stat_df[['pixel_values', label_col]].loc[:14].to_dict(orient='records')\n",
    "# valid_data = stat_df[['pixel_values', label_col]].loc[15:].to_dict(orient='records')\n",
    "\n",
    "train_data = get_data_set(os.path.join(\"TRAIN_IMAGES/\", label_col), sample_type=\"train\", transform=feature_extractor)\n",
    "valid_data = get_data_set(os.path.join(\"TRAIN_IMAGES/\", label_col), sample_type=\"validation\", transform=feature_extractor)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        # 'labels': torch.tensor([x[label_col] for x in batch])\n",
    "        'labels': torch.tensor([x['labels'] for x in batch])\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed!\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=os.path.join(\"TRAIN_IMAGES/\", label_col), drop_labels=False)\n",
    "\n",
    "# dataset = load_dataset(\"imagefolder\", data_dir=\"I:/TRAIN_IMAGES/\", split=\"train\")\n",
    "# dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100029"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.dataset import AIECVDataSet\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "train_dataset = AIECVDataSet(csv_file=\"data/TRAIN_images_metadata.csv\", root_dir=\"I:\\Images\", label_col=label_col, transform=feature_extractor)\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# for i in range(10):\n",
    "#     idx = random.randint(0, len(train_dataset))\n",
    "#     image, class_name = train_dataset[idx]\n",
    "#     ax=plt.subplot(2,5,i+1) # create an axis\n",
    "#     ax.title.set_text(class_name) # create a name of the axis based on the img name\n",
    "#     #The final tensor arrays will be of the form (C * H * W), instead of the original (H * W * C), \n",
    "#     # hence use permute to change the order\n",
    "#     plt.imshow(image.permute(1, 2, 0)) # show the img\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<src.dataset.AIECVDataSet object at 0x0000026382A8B280>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform_image(image_files):\n",
    "    inputs = feature_extractor([x.convert(\"RGB\") for x in image_files['image']], return_tensors='pt')\n",
    "    inputs['labels'] = image_files['label']\n",
    "    return inputs\n",
    "\n",
    "prep_ds = dataset.with_transform(transform_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=2351x15950>,\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    # return accuracy_score(y_true = labels, y_pred = predictions)\n",
    "    return {\n",
    "            \"f1\": float(\n",
    "                f1_score(y_true = labels, y_pred = predictions)\n",
    "            )\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\bhegde/.cache\\huggingface\\hub\\models--google--vit-base-patch16-224-in21k\\snapshots\\7cbdb7ee3a6bcdf99dae654893f66519c480a0f8\\config.json\n",
      "Model config ViTConfig {\n",
      "  \"_name_or_path\": \"google/vit-base-patch16-224-in21k\",\n",
      "  \"architectures\": [\n",
      "    \"ViTModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"No\",\n",
      "    \"1\": \"Yes\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"No\": 0,\n",
      "    \"Yes\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\bhegde/.cache\\huggingface\\hub\\models--google--vit-base-patch16-224-in21k\\snapshots\\7cbdb7ee3a6bcdf99dae654893f66519c480a0f8\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=len(labels_lst),\n",
    "    id2label={v : k for k,v in labels_lst.items()},\n",
    "    label2id = labels_lst,\n",
    "    proxies={'https': 'proxy-ir.intel.com:912'}\n",
    ")\n",
    "\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./vit-base-AIE-sample\",\n",
    "  per_device_train_batch_size=16,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=10,\n",
    "  # fp16=True,\n",
    "  save_steps=100,\n",
    "  eval_steps=100,\n",
    "  logging_steps=10,\n",
    "  learning_rate=1e-6,\n",
    "  save_total_limit=2,\n",
    "  remove_unused_columns=False,\n",
    "  push_to_hub=False,\n",
    "  report_to='tensorboard',\n",
    "  load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prep_ds[\"train\"],\n",
    "    eval_dataset=prep_ds[\"validation\"],\n",
    "    tokenizer=feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 88\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 60\n",
      "  Number of trainable parameters = 85800194\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "762ae9210a6a465088ab5717e54967fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bhegde\\codes\\MSOAInternGang\\aie_venv\\lib\\site-packages\\PIL\\Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6958, 'learning_rate': 8.333333333333333e-07, 'epoch': 1.67}\n",
      "{'loss': 0.6907, 'learning_rate': 6.666666666666666e-07, 'epoch': 3.33}\n",
      "{'loss': 0.679, 'learning_rate': 5e-07, 'epoch': 5.0}\n",
      "{'loss': 0.6753, 'learning_rate': 3.333333333333333e-07, 'epoch': 6.67}\n",
      "{'loss': 0.673, 'learning_rate': 1.6666666666666665e-07, 'epoch': 8.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./vit-base-AIE-sample\n",
      "Configuration saved in ./vit-base-AIE-sample\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.671, 'learning_rate': 0.0, 'epoch': 10.0}\n",
      "{'train_runtime': 1200.9146, 'train_samples_per_second': 0.733, 'train_steps_per_second': 0.05, 'train_loss': 0.6807907978693645, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./vit-base-AIE-sample\\pytorch_model.bin\n",
      "Image processor saved in ./vit-base-AIE-sample\\preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       10.0\n",
      "  train_loss               =     0.6808\n",
      "  train_runtime            = 0:20:00.91\n",
      "  train_samples_per_second =      0.733\n",
      "  train_steps_per_second   =       0.05\n"
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 56\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f8eed9b3ab4cc5a010ca26adc96db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =       10.0\n",
      "  eval_f1                 =     0.0769\n",
      "  eval_loss               =     0.7544\n",
      "  eval_runtime            = 0:00:32.71\n",
      "  eval_samples_per_second =      1.712\n",
      "  eval_steps_per_second   =      0.214\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate(prep_ds[\"test\"])\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aie_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "788814ecf7b2e71f9aac606fb3eee6b860c44d0c5a650a586b0a505ba47499ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
